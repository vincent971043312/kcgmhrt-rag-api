import os
import json
import shutil
import hashlib
import argparse
import inspect
try:
    # Load .env if available (non-fatal if missing)
    from dotenv import load_dotenv, find_dotenv  # type: ignore
    load_dotenv(find_dotenv(), override=False)
except Exception:
    # Fallback: basic .env parser (KEY=VALUE lines only)
    env_path = os.path.join(os.getcwd(), ".env")
    if os.path.exists(env_path):
        try:
            with open(env_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#") or "=" not in line:
                        continue
                    k, v = line.split("=", 1)
                    k = k.strip()
                    v = v.strip().strip('"').strip("'")
                    os.environ.setdefault(k, v)
        except Exception:
            pass

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document

# ========= è¨­å®š =========
# å¯ç”¨ç’°å¢ƒè®Šæ•¸è¦†å¯«ï¼ˆé©åˆ Heroku/é›²ç«¯ï¼‰ï¼š
#   DOCS_DIRï¼šä¾†æºæ–‡ä»¶ç›®éŒ„ï¼ˆé è¨­ docsï¼‰
#   DB_DIRï¼šå‘é‡è³‡æ–™åº«è·¯å¾‘ï¼ˆé è¨­ dbï¼›Heroku è«‹è¨­ç‚º /tmp/dbï¼‰
#   COLLECTION_NAMEï¼šChroma é›†åˆå‰ç¶´ï¼ˆé è¨­ my_rag_dbï¼‰
DOCS_DIR = os.getenv("DOCS_DIR", "docs")          # æ”¾æ–‡ä»¶çš„è³‡æ–™å¤¾
DB_DIR = os.getenv("DB_DIR", "db")                # å‘é‡è³‡æ–™åº«è·¯å¾‘
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "my_rag_db")

# ========= é©—è­‰é‡‘é‘° =========
if not os.getenv("OPENAI_API_KEY"):
    raise RuntimeError(
        "ç¼ºå°‘ OPENAI_API_KEYã€‚è«‹åœ¨ç’°å¢ƒè®Šæ•¸æˆ– .env ä¸­è¨­å®šï¼Œä¾‹å¦‚ï¼š\n"
        "1) åŒ¯å‡ºï¼šexport OPENAI_API_KEY=your_key\n"
        "2) æˆ–å»ºç«‹ .env æª”ï¼šOPENAI_API_KEY=your_key"
    )

def _supported_files():
    files = []
    for file in os.listdir(DOCS_DIR):
        # è·³é Windows ä¸‹è¼‰æª”æ¡ˆå¸¸è¦‹çš„é™„å¸¶è³‡æ–™æª” (Alternate Data Streams)
        if ":" in file or file.endswith("Zone.Identifier"):
            print(f"â­ï¸ è·³éé™„å¸¶è³‡æ–™æª”: {file}")
            continue
        lower = file.lower()
        if lower.endswith((".txt", ".pdf", ".md")):
            files.append(file)
        else:
            print(f"âš ï¸ ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file}")
    return sorted(files)


def _compute_manifest(files):
    manifest = []
    for f in files:
        p = os.path.join(DOCS_DIR, f)
        try:
            st = os.stat(p)
            manifest.append({
                "name": f,
                "size": st.st_size,
                "mtime": int(st.st_mtime),
            })
        except FileNotFoundError:
            continue
    return manifest


def _manifests_equal(a, b):
    if len(a) != len(b):
        return False
    a_sorted = sorted(a, key=lambda x: x["name"])
    b_sorted = sorted(b, key=lambda x: x["name"])
    return a_sorted == b_sorted


def _safe_stem(filename: str) -> str:
    """Return an ASCII-only, Chroma-safe stem.

    Rules (to satisfy Chroma collection name constraints):
    - Only allow [A-Za-z0-9._-]
    - Must start and end with [A-Za-z0-9]
    - Length between 3 and 512 (we cap at 100 for path brevity)
    - Fallback to 'doc' if empty/invalid
    """
    import re
    import unicodedata

    base = os.path.splitext(os.path.basename(filename))[0]
    # Normalize and drop non-ASCII
    norm = unicodedata.normalize('NFKD', base).encode('ascii', 'ignore').decode('ascii')
    # Replace invalid chars with underscore
    safe = re.sub(r"[^A-Za-z0-9._-]", "_", norm)
    # Trim leading/trailing non-alnum to satisfy start/end rule
    safe = re.sub(r"^[^A-Za-z0-9]+", "", safe)
    safe = re.sub(r"[^A-Za-z0-9]+$", "", safe)
    # Collapse consecutive invalid artifacts like multiple underscores or dots around dashes (optional)
    safe = re.sub(r"_{2,}", "_", safe)
    if not safe or len(safe) < 3:
        safe = "doc"
    return safe[:100]


def _make_chat_llm(max_tokens: int = 512) -> ChatOpenAI:
    """å»ºç«‹ ChatOpenAIï¼›è‹¥æ”¯æ´ `max_tokens` å‰‡ç›´æ¥å‚³å…¥ï¼Œå¦å‰‡é€€å› `model_kwargs`ã€‚
    é€™å¯é¿å…ã€è«‹é¡¯å¼æŒ‡å®šåƒæ•¸ã€çš„è­¦å‘Šï¼ŒåŒæ™‚ç›¸å®¹èˆŠç‰ˆå‹åˆ¥æª¢æŸ¥ã€‚
    """
    kwargs: dict = {"model": "gpt-4o", "temperature": 0}
    try:
        params = inspect.signature(ChatOpenAI).parameters
    except Exception:
        params = {}
    if "max_tokens" in params:
        kwargs["max_tokens"] = max_tokens
    else:
        kwargs["model_kwargs"] = {"max_tokens": max_tokens}
    return ChatOpenAI(**kwargs)

# ========= è¼‰å…¥æ–‡ä»¶ï¼ˆä¿ç•™é ç¢¼/ä¾†æºæ–¼ metadataï¼‰ =========
def load_documents():
    all_docs = []
    for file in _supported_files():
        file_path = os.path.join(DOCS_DIR, file)
        lower = file.lower()
        if lower.endswith(".txt"):
            loader = TextLoader(file_path, encoding="utf-8")
        elif lower.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif lower.endswith(".md"):
            loader = UnstructuredMarkdownLoader(file_path)
        else:
            continue

        raw_docs = loader.load()
        for d in raw_docs:
            meta = dict(d.metadata or {})
            meta["source"] = file  # ä¿ç•™ä¾†æºæª”å
            all_docs.append(
                Document(
                    page_content=d.page_content,
                    metadata=meta,
                )
            )
    return all_docs

"""
èˆŠç‰ˆä¸€æ¬¡æ€§å»ºç«‹æ•´å€‹ docs/ çš„è³‡æ–™åº«å·²ç§»é™¤éœ€æ±‚ã€‚
æ”¹ç‚ºï¼šæ¯å€‹æª”æ¡ˆå„è‡ªå»ºç«‹åˆ° db/<safe_stem>/ ä¸‹ã€‚
ä¿ç•™ load_documents() åƒ…ä¾›æœªä¾†éœ€è¦æ™‚ä½¿ç”¨ã€‚
"""


# ========= é‡å°å–®ä¸€æª”æ¡ˆå»ºç«‹ / è¼‰å…¥è³‡æ–™åº« =========
def build_or_load_db_for_file(file: str, force: bool = False) -> Chroma:
    # é©—è­‰æª”æ¡ˆ
    files = _supported_files()
    if file not in files:
        raise ValueError(f"æª”æ¡ˆä¸å­˜åœ¨æˆ–ä¸æ”¯æ´: {file}")

    # åµŒå…¥è¨­å®šï¼šå°æ‰¹æ¬¡é¿å…è¶…é OpenAI å–®è«‹æ±‚ token ä¸Šé™
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        chunk_size=128,
    )

    safe = _safe_stem(file)
    # è‹¥å¤šå€‹æª”æ¡ˆæ¸…ç†å¾ŒåŒåï¼Œç‚ºé¿å…è¡çªï¼Œé™„åŠ çŸ­é›œæ¹Š
    try:
        files_all = _supported_files()
        collisions = [f for f in files_all if _safe_stem(f) == safe]
    except Exception:
        collisions = [file]
    if len(collisions) > 1:
        short = hashlib.sha1(file.encode('utf-8')).hexdigest()[:8]
        safe = f"{safe}-{short}"
    subdir = os.path.join(DB_DIR, safe)
    os.makedirs(subdir, exist_ok=True)
    try:
        os.chmod(subdir, 0o755)
    except Exception:
        pass
    manifest_path = os.path.join(subdir, "manifest.json")
    collection = f"{COLLECTION_NAME}_{safe}"

    new_manifest = _compute_manifest([file])

    # å¼·åˆ¶é‡å»º
    if force:
        print(f"â™»ï¸ é‡æ–°å»ºç«‹ï¼ˆå¼·åˆ¶ï¼‰{file} çš„è³‡æ–™åº«...")
        shutil.rmtree(subdir, ignore_errors=True)
        os.makedirs(subdir, exist_ok=True)
    # è‹¥ç›®éŒ„å·²æœ‰å…§å®¹ä¸” manifest æœªè®Šæ›´ï¼Œç›´æ¥è¼‰å…¥
    elif any(os.scandir(subdir)):
        old_manifest = []
        if os.path.exists(manifest_path):
            try:
                with open(manifest_path, "r", encoding="utf-8") as f:
                    old_manifest = json.load(f)
            except Exception:
                old_manifest = []
        if _manifests_equal(old_manifest, new_manifest):
            print(f"ğŸ”„ è¼‰å…¥æ—¢æœ‰è³‡æ–™åº«ï¼ˆ{file}ï¼‰...")
            return Chroma(
                collection_name=collection,
                embedding_function=embeddings,
                persist_directory=subdir,
            )
        else:
            print(f"â™»ï¸ åµæ¸¬åˆ° {file} æœ‰è®Šæ›´ï¼Œé‡æ–°å»ºç«‹è©²è³‡æ–™åº«...")
            shutil.rmtree(subdir, ignore_errors=True)
            os.makedirs(subdir, exist_ok=True)

    # å»ºç«‹æ–°è³‡æ–™åº«ï¼ˆåƒ…æ­¤æª”æ¡ˆï¼‰
    print(f"ğŸ“‚ å»ºç«‹æ–°è³‡æ–™åº«ï¼ˆ{file}ï¼‰...")
    path = os.path.join(DOCS_DIR, file)
    lower = file.lower()
    if lower.endswith('.txt'):
        loader = TextLoader(path, encoding='utf-8')
    elif lower.endswith('.pdf'):
        loader = PyPDFLoader(path)
    elif lower.endswith('.md'):
        loader = UnstructuredMarkdownLoader(path)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file}")

    raw_docs = loader.load()
    docs = []
    for d in raw_docs:
        meta = dict(d.metadata or {})
        meta["source"] = file
        docs.append(Document(page_content=d.page_content, metadata=meta))

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "\n", "ï¼Œ", "ã€", " "]
    )
    docs = splitter.split_documents(docs)

    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,  # from_documents ä½¿ç”¨ embedding
        collection_name=collection,
        persist_directory=subdir,
    )

    try:
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(new_manifest, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    print(f"âœ… {file} çš„è³‡æ–™åº«å»ºç«‹å®Œæˆ")
    return vectorstore


def run_chat_for_file(file: str):
    print(f"ğŸ“š ç›®å‰èŠå¤©æª”æ¡ˆï¼š{file}")
    vectorstore = build_or_load_db_for_file(file)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    qa = RetrievalQA.from_chain_type(
        llm=_make_chat_llm(max_tokens=512),
        retriever=retriever,
    )

    print("\nğŸ¤– RAG å•ç­”ç³»çµ±å·²å•Ÿå‹•ï¼ï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼›è¼¸å…¥ /help çœ‹æŒ‡ä»¤ï¼‰")
    while True:
        query = input("\nâ“ è¼¸å…¥ä½ çš„å•é¡Œ: ").strip()
        if not query:
            continue
        low = query.lower()

        # æ§åˆ¶æŒ‡ä»¤
        if low in {"exit", ":q", "/exit", ":exit"}:
            break
        if low in {"/help", ":help", "help"}:
            print("\næŒ‡ä»¤ï¼š\n  /list              åˆ—å‡ºå¯ç”¨æª”æ¡ˆ\n  /switch <æª”æˆ–åºè™Ÿ> åˆ‡æ›èŠå¤©æª”æ¡ˆ\n  /reload            é‡æ–°å»ºç«‹ç›®å‰æª”æ¡ˆç´¢å¼•\n  /current           é¡¯ç¤ºç›®å‰èŠå¤©æª”æ¡ˆ\n  exit               é›¢é–‹\n")
            continue
        if low in {"/current", ":current"}:
            print(f"ğŸ“š ç›®å‰èŠå¤©æª”æ¡ˆï¼š{file}")
            continue
        if low in {"/list", ":list", "/files", ":files"}:
            files = _supported_files()
            if files:
                print("ğŸ“„ å¯ç”¨æª”æ¡ˆï¼š")
                for i, f in enumerate(files, 1):
                    print(f"  {i}. {f}")
            else:
                print("ğŸ“­ docs/ ç›®å‰æ²’æœ‰å¯æ”¯æ´çš„æª”æ¡ˆï¼ˆæ”¯æ´ .pdf/.txt/.mdï¼‰")
            continue
        if low.startswith("/switch") or low.startswith(":switch") or low.startswith("/open") or low.startswith(":open"):
            parts = query.split(maxsplit=1)
            target = None
            files = _supported_files()
            if len(parts) == 2:
                arg = parts[1].strip()
                if arg.isdigit() and 1 <= int(arg) <= len(files):
                    target = files[int(arg) - 1]
                elif arg in files:
                    target = arg
            if not target:
                print("âŒ è«‹æä¾›æœ‰æ•ˆçš„æª”åæˆ–åºè™Ÿï¼ˆå…ˆç”¨ /list æŸ¥è©¢ï¼‰ã€‚")
                continue
            # åˆ‡æ›
            file = target
            print(f"ğŸ” åˆ‡æ›è‡³ï¼š{file}")
            vectorstore = build_or_load_db_for_file(file)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            qa = RetrievalQA.from_chain_type(
                llm=_make_chat_llm(max_tokens=512),
                retriever=retriever,
            )
            continue
        if low in {"/reload", ":reload"}:
            print(f"â™»ï¸ é‡æ–°å»ºç«‹ç´¢å¼•ï¼š{file}")
            vectorstore = build_or_load_db_for_file(file, force=True)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            qa = RetrievalQA.from_chain_type(
                llm=_make_chat_llm(max_tokens=512),
                retriever=retriever,
            )
            continue

        # ä¸€èˆ¬å•ç­”
        answer = qa.run(query)
        print(f"ğŸ‘‰ ç­”æ¡ˆ: {answer}")

# ========= å•ç­”ç³»çµ± =========
def start_chat():
    # äº’å‹•å¼é¸æ“‡å–®ä¸€æª”æ¡ˆå†èŠå¤©
    files = _supported_files()
    if not files:
        print("ğŸ“­ docs/ ç›®å‰æ²’æœ‰å¯æ”¯æ´çš„æª”æ¡ˆï¼ˆæ”¯æ´ .pdf/.txt/.mdï¼‰")
        return
    print("ğŸ“„ å¯ç”¨æª”æ¡ˆï¼š")
    for i, f in enumerate(files, 1):
        print(f"  {i}. {f}")
    choice = input("è«‹è¼¸å…¥è¦èŠå¤©çš„æª”æ¡ˆåºè™Ÿï¼ˆæˆ–ç›´æ¥è¼¸å…¥æª”åï¼‰: ").strip()
    if choice.isdigit() and 1 <= int(choice) <= len(files):
        target = files[int(choice) - 1]
    else:
        if choice in files:
            target = choice
        else:
            print("âŒ è¼¸å…¥ç„¡æ•ˆï¼ŒçµæŸã€‚")
            return
    run_chat_for_file(target)


def preindex_all(
    pdf_only: bool = True,
    force: bool = False,
    offset=None,
    limit=None,
    batch_size=None,
    batch_index=None,
):
    files = _supported_files()
    if pdf_only:
        files = [f for f in files if f.lower().endswith('.pdf')]
    total = len(files)
    if not files:
        print("ğŸ“­ æ²’æœ‰ç¬¦åˆçš„æª”æ¡ˆå¯å»ºç«‹ç´¢å¼•")
        return

    selected_info = ""
    # å„ªå…ˆä½¿ç”¨ offset/limitï¼ˆè¼ƒç›´è§€ï¼‰
    if offset is not None or limit is not None:
        off = max(0, offset or 0)
        if limit is None:
            files = files[off:]
        else:
            if limit < 0:
                print("âŒ --limit éœ€ç‚ºéè² æ•´æ•¸")
                return
            files = files[off: off + limit]
        if not files:
            print("ğŸ“­ ä¾ offset/limit æ²’æœ‰å¯è™•ç†çš„æª”æ¡ˆ")
            return
        selected_info = f"offset={off}, limit={limit if limit is not None else 'âˆ'}"
    # å¦å‰‡ä½¿ç”¨ batch-size/batch-index
    elif batch_size is not None or batch_index is not None:
        if batch_size is None or batch_index is None:
            print("âŒ åˆ†æ‰¹éœ€åŒæ™‚æä¾› --batch-size èˆ‡ --batch-index")
            return
        if batch_size <= 0 or batch_index <= 0:
            print("âŒ --batch-size èˆ‡ --batch-index éœ€ç‚ºæ­£æ•´æ•¸")
            return
        num_batches = (total + batch_size - 1) // batch_size
        if batch_index > num_batches:
            print(f"âŒ æ‰¹æ¬¡ç·¨è™Ÿè¶…å‡ºç¯„åœï¼šå…± {num_batches} æ‰¹ï¼Œæ”¶åˆ° {batch_index}")
            return
        start = (batch_index - 1) * batch_size
        end = min(total, start + batch_size)
        files = files[start:end]
        if not files:
            print("ğŸ“­ æ­¤æ‰¹æ¬¡æ²’æœ‰å¯è™•ç†çš„æª”æ¡ˆ")
            return
        selected_info = f"æ‰¹æ¬¡ {batch_index}/{num_batches}ï¼ˆç´¢å¼• {start}..{end-1}ï¼‰"

    print("ğŸš€ é–‹å§‹å»ºç«‹ç´¢å¼•ï¼ˆ{}{}ï¼‰...".format(
        "åƒ… PDF" if pdf_only else "å…¨éƒ¨æ”¯æ´æª”æ¡ˆ",
        f"ï¼›{selected_info}" if selected_info else ""
    ))
    success, failed = 0, []
    for i, f in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] æª”æ¡ˆï¼š{f}")
        try:
            build_or_load_db_for_file(f, force=force)
            success += 1
        except Exception as e:
            print(f"âŒ å¤±æ•—ï¼š{e}")
            failed.append((f, str(e)))
    print("\nâœ… å®Œæˆï¼š{} æˆåŠŸï¼Œ{} å¤±æ•—".format(success, len(failed)))
    if failed:
        print("å¤±æ•—æ¸…å–®ï¼š")
        for f, msg in failed:
            print(" -", f, "=>", msg)


def main():
    parser = argparse.ArgumentParser(description="Per-file RAG for docs/")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨æª”æ¡ˆ")
    parser.add_argument("--index", type=str, help="åƒ…ç‚ºæŒ‡å®šæª”æ¡ˆå»ºç«‹/æ›´æ–°ç´¢å¼•å¾ŒçµæŸ")
    parser.add_argument("--chat", type=str, help="ç‚ºæŒ‡å®šæª”æ¡ˆå»ºç«‹/æ›´æ–°ç´¢å¼•å¾Œé€²å…¥èŠå¤©")
    parser.add_argument("--preindex", action="store_true", help="å…ˆç‚º docs/ å…§æ‰€æœ‰ PDF å»ºç«‹/æ›´æ–°ç´¢å¼•å¾ŒçµæŸ")
    parser.add_argument("--preindex-all", action="store_true", help="å…ˆç‚º docs/ å…§æ‰€æœ‰æ”¯æ´æª”æ¡ˆå»ºç«‹/æ›´æ–°ç´¢å¼•å¾ŒçµæŸ")
    parser.add_argument("--force", action="store_true", help="æ­é… --preindex* å¼·åˆ¶é‡å»ºç´¢å¼•")
    # åˆ†æ‰¹/åˆ†æ®µåƒæ•¸ï¼ˆæ­é… --preindex* ä½¿ç”¨ï¼‰
    parser.add_argument("--offset", type=int, default=None, help="å¾æ­¤ç´¢å¼•é–‹å§‹ï¼ˆ0-basedï¼‰")
    parser.add_argument("--limit", type=int, default=None, help="æœ€å¤šè™•ç†å¹¾å€‹æª”æ¡ˆ")
    parser.add_argument("--batch-size", type=int, default=None, help="æ¯æ‰¹è™•ç†å¹¾å€‹æª”æ¡ˆ")
    parser.add_argument("--batch-index", type=int, default=None, help="è¦è™•ç†ç¬¬å¹¾æ‰¹ï¼ˆ1-basedï¼‰")
    args = parser.parse_args()

    files = _supported_files()
    if args.preindex or args.preindex_all:
        preindex_all(
            pdf_only=not args.preindex_all,
            force=args.force,
            offset=args.offset,
            limit=args.limit,
            batch_size=args.batch_size,
            batch_index=args.batch_index,
        )
        return
    if args.list:
        if files:
            print("ğŸ“„ å¯ç”¨æª”æ¡ˆï¼š")
            for f in files:
                print(" -", f)
        else:
            print("ğŸ“­ docs/ ç›®å‰æ²’æœ‰å¯æ”¯æ´çš„æª”æ¡ˆï¼ˆæ”¯æ´ .pdf/.txt/.mdï¼‰")
        return

    if args.index:
        if args.index not in files:
            print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆæˆ–ä¸æ”¯æ´ï¼š", args.index)
            return
        build_or_load_db_for_file(args.index)
        return

    if args.chat:
        if args.chat not in files:
            print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆæˆ–ä¸æ”¯æ´ï¼š", args.chat)
            return
        run_chat_for_file(args.chat)
        return

    # ç„¡åƒæ•¸ï¼šèµ°äº’å‹•å¼é¸æ“‡
    start_chat()

if __name__ == "__main__":
    main()
